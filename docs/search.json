[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Akhilesh Chegu: I am a freshman from San Jose, California, and I am intending to major in Neuroscience.\nWilliam Yan: I am a freshman from Plano, Texas, and I am intending to major in Biology.\nAnthony Tarakji: I am a Senior from Los Angeles, California. I am majoring in Neuroscience, and minoring in Chemistry. I plan on attending Medical School.\nAudrey Patterson: I am a freshmen from New York City. I am intending to major in Public Policy."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TEAM NAME",
    "section": "",
    "text": "Add project abstract here."
  },
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "Tean AAAW Proposal",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "proposal.html#introduction-and-data",
    "href": "proposal.html#introduction-and-data",
    "title": "Tean AAAW Proposal",
    "section": "Introduction and data",
    "text": "Introduction and data\n\nIdentify the source of the data\nThe COVID Tracking Project is collecting this data.\nState when and how it was originally collected (by the original data curator, not necessarily how you found the data).\nThis data is taken from the websites of state/territory public health authorities. The website states that when there are holes in the data, they use information from official state social media accounts and official announcements and statements by governors or other authorities. Some state’s public health dept pages had covid specific pages that had this information.\nWrite a brief description of the observations.\nAs time progressed throughout the pandemic, many more tests began being administered. This is evident in the new tests column as the amount of new tests  has a general upward trend. The correlation between new cases (positive tests) and negative PCR tests seems relatively consistent over time. It seems that there are usually 1/3-1/2 the amount of positive tests as there are negative tests. I see that there was a spike in hospitalization in December of 2020. \n\nThe COVID Tracking Project is collecting this data \n\nState when and how it was originally collected (by the original data curator, not necessarily how you found the data).\n\nThis data is taken from the websites of state/territory public health authorities. The website states that when there are holes in the data, they use information from official state social media accounts and official announcements and statements by governors or other authorities. Some state’s public health dept pages had covid specific pages that had this information.\n\nWrite a brief description of the observations.\n\nEach observation is a day from Jan 13, 2020 to March 7, 2021 and it follows data in the US for COVID tests, positive/negative results, and hospitalization data across the country.\nAs time progressed throughout the pandemic, many more tests began being administered. This is evident in the new tests column as the amount of new tests has a general upward trend. The correlation between new cases (positive tests) and negative PCR tests seems relatively consistent over time. It seems that there are usually 1/3-1/2 the amount of positive tests as there are negative tests. I see that there was a spike in hospitalization in December of 2020. \n\nThere aren’t any major ethical concerns."
  },
  {
    "objectID": "proposal.html#research-question",
    "href": "proposal.html#research-question",
    "title": "Tean AAAW Proposal",
    "section": "Research question",
    "text": "Research question\n\nResearch Question:\nIs there a significant correlation between new tests administered on a certain day and deaths on that same day and each day up to two weeks after?\nOn what day was there the highest amount of positive tests? And in the four weeks following, was the number of deaths greater than average?\n\nThe first question is important because there were a lot of controversies raised during the pandemic about testings correlation with death rates. Many believed that mass testing was an effective way to reduce death rates. Many believed that testing was unnecessary and was not a beneficial use of resources. This question would answer if the amount of tests administered impact the amount of COVID deaths and if increased testing reduces the death rate over a span of the two week contagion period. \nThe second question responds to a similar controversy. Many were arguing that positive cases did not correlate with death rates and that there shouldn’t be work done to decrease the amount of positive tests because it wouldn’t have a significant health impact on the population. This question would follow the death rate the highest positive test day for the four weeks following that date. This is the time span that most die of the illness after contracting it. If the people arguing against intervention are correct, the average deaths for those days following the greatest positive test day won’t be greater than that of the rest of the data for all the other days. \n\nA description of the research topic along with a concise statement of your hypotheses on this topic?\nThe research topic that we are addressing with this data set is an analysis of common COVID controversies. We are attempting to discern if death rates had any correlation with testing rates. \n\nIs there a significant correlation between new tests administered on a certain day, positive tests, new hospitalization, and deaths on that same day and each day up to two weeks after?\nOn what day was there the highest amount of positive tests? And in the four weeks following, was the number of deaths greater than average? \n\nImportance:\n\nThe first question is important because there were a lot of controversies raised during the pandemic about testings correlation with death rates. Many believed that mass testing was an effective way to reduce death rates. Many believed that testing was unnecessary and was not a beneficial use of resources. This question would answer if the amount of tests administered impact the amount of COVID deaths and if increased testing reduces the death rate over a span of the two week contagion period.\nThe second question responds to a similar controversy. Many were arguing that positive cases did not correlate with death rates and that there shouldn’t be work done to decrease the amount of positive tests because it wouldn’t have a significant health impact on the population. This question would follow the death rate the highest positive test day for the four weeks following that date. This is the time span that most die of the illness after contracting it. If the people arguing against intervention are correct, the average deaths for those days following the greatest positive test day won’t be greater than that of the rest of the data for all the other days. \n\nDescription and Hypothesis\n\nThe research topic that we are addressing with this data set is an analysis of common COVID controversies. We are attempting to discern if death rates had any correlation with testing rates. \nFor the first question, we hypothesize that there will be a significant positive correlation between new tests administered on a certain day and deaths on that same day and each day up to two weeks after.\nFor the second question, we hypothesize that the highest amount of positive test would be towards the end of the dataset (Jan - Feb 2021) based on personal experience going through the year of 2021.\n\nCategorical: There is no categorical data for this set. Quantitative: number of deaths a day, number of tests administered a day, number of positive tests, number of negative tests, hospitalization number\n\nCategorical: There is no categorical data for this set \nQuantitative: number of deaths a day, number of tests administered a day, number of positive tests, number of negative tests, hospitalization numbers"
  },
  {
    "objectID": "proposal.html#literature",
    "href": "proposal.html#literature",
    "title": "Tean AAAW Proposal",
    "section": "Literature",
    "text": "Literature\n\nPublished article\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC7302472/pdf/mm6924e2.pdf\nProvide a one paragraph summary about the article.\n\nThis case surveillance report tracked all US COVID cases from January 22-May 30th of 2020. This report dives more into the demographics of those testing positive for COVID vs the nature of our data set which just gives the exact numerical reports for amount of deaths/tests/etc. This article specifically focuses on underlying conditions that COVID patients had and how that correspond to the severity of their illness (hospitalization vs intensive care vs those who died). It explains the median age of those contracting COVID. It also explains who was most likely to experience the illness asymptomatically. It provides data tables to discern the exact amount of people with pre-existing conditions and sorts them by sex and age. There is a data table reporting cumulative incidence by sex and age. There is also a graph that presents the number of cases and another one with the number of deaths during that same time frame. \n\nIn 1-2 sentences, explain how your research question builds on / is different than the article you have cited.\n\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC7302472/pdf/mm6924e2.pdf\n\nSummary\n\nThis case surveillance report tracked all US COVID cases from January 22-May 30th of 2020. This report dives more into the demographics of those testing positive for COVID vs the nature of our data set which just gives the exact numerical reports for amount of deaths/tests/etc. This article specifically focuses on underlying conditions that COVID patients had and how that correspond to the severity of their illness (hospitalization vs intensive care vs those who died). It explains the median age of those contracting COVID. It also explains who was most likely to experience the illness asymptomatically. It provides data tables to discern the exact amount of people with pre-existing conditions and sorts them by sex and age. There is a data table reporting cumulative incidence by sex and age. There is also a graph that presents the number of cases and another one with the number of deaths during that same time frame. This article, although covering more specificities, does not present monthly testing numbers throughout the peak of the pandemic.\n\nConnection\n\nThis report explains the demographics and the underlying conditions of the patients, but does not give any timeline for the AMOUNT of people testing positive. It also gives no information about the amount of people getting tested or amount of those who test negative. It only explains the demographics of those who test positive and then what happens to them post positive test. The changing of testing, hospitalization, and death numbers is what our data set adds on to the conversation. \n\nThis article, although covering more specificities, does not present monthly testing numbers throughout the peak of the pandemic. This report explains the demographics and the underlying conditions of the patients, but does not give any timeline for the AMOUNT of people testing positive. It also gives no information about the amount of people getting tested or amount of those who test negative. It only explains the demographics of those who test positive and then what happens to them post positive test. The changing of testing, hospitalization, and death numbers is what our data set adds on to the conversation."
  },
  {
    "objectID": "proposal.html#glimpse-of-data",
    "href": "proposal.html#glimpse-of-data",
    "title": "Tean AAAW Proposal",
    "section": "Glimpse of data",
    "text": "Glimpse of data\n\nAAAW_national_covid_history <- read_csv(\"data/AAAW_national-covid-history.csv\")\n\nRows: 420 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (16): death, deathIncrease, inIcuCumulative, inIcuCurrently, hospitaliz...\ndate  (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(AAAW_national_covid_history)\n\nRows: 420\nColumns: 17\n$ date                     <date> 2021-03-07, 2021-03-06, 2021-03-05, 2021-03-…\n$ death                    <dbl> 515151, 514309, 512629, 510408, 508665, 50621…\n$ deathIncrease            <dbl> 842, 1680, 2221, 1743, 2449, 1728, 1241, 1051…\n$ inIcuCumulative          <dbl> 45475, 45453, 45373, 45293, 45214, 45084, 449…\n$ inIcuCurrently           <dbl> 8134, 8409, 8634, 8970, 9359, 9465, 9595, 980…\n$ hospitalizedIncrease     <dbl> 726, 503, 2781, 1530, 2172, 1871, 1024, 879, …\n$ hospitalizedCurrently    <dbl> 40199, 41401, 42541, 44172, 45462, 46388, 467…\n$ hospitalizedCumulative   <dbl> 776361, 775635, 775132, 772351, 770821, 76864…\n$ negative                 <dbl> 74582825, 74450990, 74307155, 74035238, 73857…\n$ negativeIncrease         <dbl> 131835, 143835, 271917, 177957, 267001, 25577…\n$ onVentilatorCumulative   <dbl> 4281, 4280, 4275, 4267, 4260, 4257, 4252, 425…\n$ onVentilatorCurrently    <dbl> 2802, 2811, 2889, 2973, 3094, 3169, 3171, 324…\n$ positive                 <dbl> 28756489, 28714654, 28654639, 28585852, 28520…\n$ positiveIncrease         <dbl> 41835, 60015, 68787, 65487, 66836, 54248, 480…\n$ states                   <dbl> 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 5…\n$ totalTestResults         <dbl> 363825123, 362655064, 361224072, 359479655, 3…\n$ totalTestResultsIncrease <dbl> 1170059, 1430992, 1744417, 1590984, 1406795, …"
  },
  {
    "objectID": "proposal.html#introduction-and-data-1",
    "href": "proposal.html#introduction-and-data-1",
    "title": "Tean AAAW Proposal",
    "section": "Introduction and data",
    "text": "Introduction and data\n\nIdentify the source of the data.\n\nThis data was collected by Robert Sunderhaft\n\nState when and how it was originally collected (by the original data curator, not necessarily how you found the data).\n\nThis data was webscraped from Basketball Reference, a large public website with player specific data and statistics, and organized into the dataset later by Robert.\n\nWrite a brief description of the observations.\n\nThe observations are every player who has played in the NBA playoffs from 1950-2022, and relevant statistics from their and their team’s playoff run, ranging from identifying information like name and age to simple statistics like field goal percentage and rebounds per game to advanced statistics like VORP and Win Shares per 48 minutes. Some data, like percentages, blocks, turnovers, and other advanced statistics, weren’t collected for players early in this dataset, so there are null values instead.\n\nThere aren’t any major ethical concerns."
  },
  {
    "objectID": "proposal.html#research-question-1",
    "href": "proposal.html#research-question-1",
    "title": "Tean AAAW Proposal",
    "section": "Research question",
    "text": "Research question\n\nResearch question\n\nHow does squad depth effect the play-off results of the NBA championship? This will consider more than 3 mutated variables (substitute count, subsitute ppg, and substitute win share).\n\nImportance\n\nDepth is the amount and quality of play by the substitutes on a given team. It is an often overlooked variable that significantly contributes to the over-all strength of the team. Performing an analysis on this question allow us to develop a systematic method to better understand and assess this often-neglected factor.\n\nDescription & hypotheses\n\nThis research aims to develop a systematic way to evaluate team depth using the NBA play-off data from 1950-2022. We will then investigate its effect on the final results of the championship. To evaluate team depth, we will look at variables such as the points by bench players, numbers of bench players, field goal percentage of role players. We will also ask questions such as are the player minutes evenly-spreaded, and are all the positions well-subsituted, to evaluate the depth of the team. Eventually, we will assign relative scores for team depth and evaluate its association with final championship results.\nWe hypothesize that the deeper the team is, the more likely it is going to get higher places because it has plenty of substitutes to get players rest.\n\nIdentify the types of variables\n\nWe will use a mix of categorical and numerical variable to answer this questions. We will look into categorical variables such as player positions and championship results to evaluate the completeness and sucess of the team. We will look into numerical variables such as player minutes, field goal percentage, and numbers of bench player to eventually create a new numerical score that describes the team’s overall depth."
  },
  {
    "objectID": "proposal.html#literature-1",
    "href": "proposal.html#literature-1",
    "title": "Tean AAAW Proposal",
    "section": "Literature",
    "text": "Literature\n\nPublished credible article:\n\n“Predicting the Outcome of NBA Playoffs Based on the Maximum Entropy Principle” (Cheng et al.)\nhttps://www.mdpi.com/1099-4300/18/12/450\n\nSummary\n\nCheng et al. developed a machine learning model that utilized maximum entropy principle to predict the outcome of nba play-off results using 14 basic technical features, such as 3 points, field goals, and number of steals. Their model reached an accuracy of 74.4%, which outperformed the previous state-of-the-art with 70% accuracy. The author reasoned that this increase of performance came from the maximum entropy principle as well as their consideration of the team’s last 6 game’s data which gave a more detailed depiction of team’s strength at the specific time before the play-off game.\n\nConnection\n\nAmong the 14 features that they considered, team depth was not included. We believe that the team depth is a great factor that impacts the result of the play-off games. And if our hypothesis is true, the team depth variable could be a valuable addition to Cheng et al.’s machine learning to improve their accuracy."
  },
  {
    "objectID": "proposal.html#glimpse-of-data-1",
    "href": "proposal.html#glimpse-of-data-1",
    "title": "Tean AAAW Proposal",
    "section": "Glimpse of data",
    "text": "Glimpse of data\n\nAAW_playoff_stats <- read_csv(\"data/AAAW_updatedplayoffStats.csv\") \n\nRows: 10648 Columns: 51\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): player, pos, team_id\ndbl (48): season, age, g, gs, mp_per_g, fg_per_g, fga_per_g, fg_pct, fg3_per...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nAAAW_playoff_stats <- read_csv(\"data/AAAW_updatedplayoffStats.csv\")\n\nRows: 10648 Columns: 51\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): player, pos, team_id\ndbl (48): season, age, g, gs, mp_per_g, fg_per_g, fga_per_g, fg_pct, fg3_per...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(AAAW_playoff_stats)\n\nRows: 10,648\nColumns: 51\n$ season           <dbl> 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022,…\n$ player           <chr> \"Omer Yurtseven\", \"Kessler Edwards\", \"Draymond Green\"…\n$ pos              <chr> \"C\", \"SF\", \"PF\", \"SF\", \"PG\", \"PF\", \"C\", \"PF\", \"SG\", \"…\n$ age              <dbl> 23, 21, 31, 34, 26, 26, 29, 33, 28, 23, 24, 28, 27, 2…\n$ team_id          <chr> \"MIA\", \"BRK\", \"GSW\", \"PHI\", \"NOP\", \"DEN\", \"UTA\", \"ATL…\n$ g                <dbl> 9, 2, 22, 12, 6, 5, 6, 5, 5, 6, 9, 18, 10, 6, 13, 4, …\n$ gs               <dbl> 0, 0, 22, 12, 0, 5, 6, 3, 0, 0, 0, 18, 10, 6, 13, 4, …\n$ mp_per_g         <dbl> 4.2, 3.5, 32.0, 26.6, 10.0, 32.0, 32.8, 22.4, 15.2, 6…\n$ fg_per_g         <dbl> 1.3, 0.0, 3.1, 3.0, 1.0, 4.6, 3.5, 3.6, 1.2, 0.0, 0.3…\n$ fga_per_g        <dbl> 2.0, 0.0, 6.5, 7.4, 3.0, 10.8, 5.5, 9.0, 3.0, 1.2, 0.…\n$ fg_pct           <dbl> 0.667, NA, 0.479, 0.404, 0.333, 0.426, 0.636, 0.400, …\n$ fg3_per_g        <dbl> 0.0, 0.0, 0.4, 2.6, 0.8, 0.6, 0.0, 0.8, 0.8, 0.0, 0.2…\n$ fg3a_per_g       <dbl> 0.2, 0.0, 1.8, 6.3, 2.3, 3.0, 0.0, 3.0, 2.2, 0.5, 0.2…\n$ fg3_pct          <dbl> 0.000, NA, 0.205, 0.408, 0.357, 0.200, NA, 0.267, 0.3…\n$ fg2_per_g        <dbl> 1.3, 0.0, 2.8, 0.4, 0.2, 4.0, 3.5, 2.8, 0.4, 0.0, 0.1…\n$ fg2a_per_g       <dbl> 1.8, 0.0, 4.8, 1.1, 0.7, 7.8, 5.5, 6.0, 0.8, 0.7, 0.2…\n$ fg2_pct          <dbl> 0.750, NA, 0.581, 0.385, 0.250, 0.513, 0.636, 0.467, …\n$ efg_pct          <dbl> 0.667, NA, 0.507, 0.579, 0.472, 0.454, 0.636, 0.444, …\n$ ft_per_g         <dbl> 0.1, 0.0, 1.4, 0.0, 1.2, 4.0, 5.0, 2.2, 0.8, 0.0, 0.0…\n$ fta_per_g        <dbl> 0.3, 0.0, 2.1, 0.1, 1.3, 5.6, 7.3, 2.2, 1.0, 0.0, 0.0…\n$ ft_pct           <dbl> 0.333, NA, 0.638, 0.000, 0.875, 0.714, 0.682, 1.000, …\n$ orb_per_g        <dbl> 0.1, 0.0, 1.2, 0.8, 0.3, 3.6, 3.5, 0.4, 0.2, 0.3, 0.0…\n$ drb_per_g        <dbl> 0.7, 0.0, 6.0, 2.3, 1.2, 3.6, 9.7, 3.8, 0.4, 0.2, 0.2…\n$ trb_per_g        <dbl> 0.8, 0.0, 7.2, 3.1, 1.5, 7.2, 13.2, 4.2, 0.6, 0.5, 0.…\n$ ast_per_g        <dbl> 0.3, 0.5, 6.3, 0.8, 0.7, 2.6, 0.5, 0.8, 1.4, 0.5, 0.0…\n$ stl_per_g        <dbl> 0.0, 0.5, 1.1, 1.0, 0.2, 0.4, 0.2, 0.2, 0.2, 0.2, 0.0…\n$ blk_per_g        <dbl> 0.1, 0.0, 1.0, 0.3, 0.2, 1.2, 1.0, 0.0, 0.0, 0.0, 0.0…\n$ tov_per_g        <dbl> 0.0, 0.5, 2.7, 1.1, 0.7, 1.6, 1.3, 1.0, 0.4, 0.2, 0.0…\n$ pf_per_g         <dbl> 0.2, 1.5, 4.0, 1.9, 0.5, 2.8, 3.2, 1.2, 1.4, 1.0, 0.0…\n$ pts_per_g        <dbl> 2.8, 0.0, 8.0, 8.6, 4.0, 13.8, 12.0, 10.2, 4.0, 0.0, …\n$ ast_pct          <dbl> 16.6, 17.5, 25.7, 4.9, 9.4, 12.0, 2.4, 6.4, 12.4, 10.…\n$ blk_pct          <dbl> 2.9, 0.0, 3.2, 1.2, 1.4, 3.9, 3.8, 0.0, 0.0, 0.0, 0.0…\n$ bpm              <dbl> 8.0, -4.4, 0.9, 1.3, -1.1, -1.3, 2.3, -1.8, -2.8, -14…\n$ dbpm             <dbl> 2.8, 6.4, 2.9, 0.9, -1.0, -2.2, 0.1, -1.3, -1.9, -2.9…\n$ drb_pct          <dbl> 19.3, 0.0, 20.6, 10.3, 14.7, 14.4, 34.1, 20.9, 3.4, 3…\n$ dws              <dbl> 0.0, 0.0, 1.0, 0.3, 0.0, 0.0, 0.2, 0.0, -0.1, 0.0, 0.…\n$ fg3a_per_fga_pct <dbl> 0.111, NA, 0.271, 0.854, 0.778, 0.278, 0.000, 0.333, …\n$ fta_per_fga_pct  <dbl> 0.167, NA, 0.326, 0.011, 0.444, 0.519, 1.333, 0.244, …\n$ mp               <dbl> 38, 7, 703, 319, 60, 160, 197, 112, 76, 36, 16, 688, …\n$ obpm             <dbl> 5.1, -10.8, -2.0, 0.4, -0.2, 0.9, 2.2, -0.6, -0.8, -1…\n$ orb_pct          <dbl> 3.0, 0.0, 4.3, 3.6, 3.7, 13.5, 12.5, 2.1, 1.6, 6.2, 0…\n$ ows              <dbl> 0.1, 0.0, 0.4, 0.0, 0.1, 0.3, 0.6, 0.0, 0.1, -0.1, 0.…\n$ per              <dbl> 25.8, -2.2, 12.3, 9.9, 11.5, 15.7, 19.4, 10.8, 7.8, -…\n$ stl_pct          <dbl> 0.0, 7.3, 1.8, 2.0, 0.9, 0.6, 0.3, 0.5, 0.7, 1.5, 0.0…\n$ tov_pct          <dbl> 0.0, 100.0, 26.4, 12.7, 15.7, 10.8, 13.3, 9.1, 10.4, …\n$ trb_pct          <dbl> 10.8, 0.0, 12.7, 7.1, 8.9, 13.9, 23.3, 11.3, 2.4, 5.1…\n$ ts_pct           <dbl> 0.647, NA, 0.534, 0.576, 0.558, 0.520, 0.688, 0.512, …\n$ usg_pct          <dbl> 22.6, 6.6, 14.0, 15.1, 18.3, 20.2, 14.3, 22.5, 11.0, …\n$ vorp             <dbl> 0.1, 0.0, 0.5, 0.3, 0.0, 0.0, 0.2, 0.0, 0.0, -0.1, 0.…\n$ ws               <dbl> 0.2, 0.0, 1.4, 0.3, 0.1, 0.2, 0.7, 0.0, 0.0, -0.2, 0.…\n$ ws_per_48        <dbl> 0.228, -0.104, 0.094, 0.049, 0.049, 0.063, 0.182, 0.0…"
  },
  {
    "objectID": "proposal.html#introduction-and-data-2",
    "href": "proposal.html#introduction-and-data-2",
    "title": "Tean AAAW Proposal",
    "section": "Introduction and data",
    "text": "Introduction and data\n\nIdentify the source of the data.\n\nNational Center For Health Statistics (NCHS)\n\nState when and how it was originally collected (by the original data curator, not necessarily how you found the data).\n\nThis data was last updated March 8, 2023 and created May 8, 2020. It is not specified how the data was collected, but it was likely by surveying COVID-19 infection and death reports that were given to the NCHS by different jurisdictions.\n\nDescription\n\nEach observation is a time start and end date for data collection as COVID-19 deaths. The variables are COVID-19 death, age group, condition, condition group, jurisdiction, year, and month during the time span.\n\nThere aren’t any major ethical concerns for this dataset."
  },
  {
    "objectID": "proposal.html#research-question-2",
    "href": "proposal.html#research-question-2",
    "title": "Tean AAAW Proposal",
    "section": "Research question",
    "text": "Research question\n\nQuestion\n\nWhat 3 groups of co-morbid conditions are most associated with mortality due to COVID-19 by age group (below 55 and above 55) and how do these compare to deaths from COVID-19 without mention of other co-morbid conditions?\n\nImportance\n\nIdentification of diseases and conditions that are most directly related to mortality from COVID-19 provides valuable information regarding how patients should be treated and which individuals should take extra precautions to avoid infection. Treatment plans can be catered to these conditions: for example, an individual that is suffering from a “dangerous” co-morbidity in the context of COVID-19 can be immediately or prophylacticly prescribed an antiviral like Nirmatrelvir/ritonavir (Pavlovid TM) to avoid complications and mortality if infected.\n\nDescription and Hypothesis\n\nOur research topic will focus on determining the 3 co-morbid condition groups that are most related to COVID-19 mortality separated by age. We will analyze how condition groups impact the incidence of COVID-19 mortality for individuals above and below the age of 55 in the United States.\nWe hypothesize that individuals above the age of 55 years will have comprise the largest group of COVID-19 mortalities. For individuals below 55, we hypothesize respiratory illness, sepsis, and obesity will be the highest risk factors for COVID-19 mortality. COVID-19 is a respiratory illness so we expect that mortality will be most likely in individuals with pre-existing respiratory conditions. Sepsis is a common fatal complication of infection. Obesity is a systemic condition, that dramatically harm treatment outcomes and responses. For individuals above the age of 55 we hypothesize respiratory illnesses will be the predominant co-morbidity because COVID-19 is a respiratory illness, Obesity, and Vascular and Unspecified Dementia will be second and third.\n\nVariables\n\nCOVID-19 deaths is quantitative while Condition Groups and age groups are Categorical."
  },
  {
    "objectID": "proposal.html#literature-2",
    "href": "proposal.html#literature-2",
    "title": "Tean AAAW Proposal",
    "section": "Literature",
    "text": "Literature\n\nArticle\n\nhttps://jamanetwork.com/journals/jama/fullarticle/2765184 . Richardson S, Hirsch JS, Narasimhan M, et al. Presenting Characteristics, Comorbidities, and Outcomes Among 5700 Patients Hospitalized With COVID-19 in the New York City Area. JAMA. 2020;323(20):2052–2059. doi:10.1001/jama.2020.6775\n\nSummary\n\nThis study analyzes the risk of hospitalization by co-morbidity for 5,700 individuals in New York City from March 1, 2020 to April 1,2020. It has been shown that the majority of individuals that are ultimately hospitalized for COVID-19 are of older age and have various co-morbid conditions. The study found that the most common co-morbid conditions among individuals hospitalized with COVID-19 are Hypertension, Obesity, and Diabetes.\n\nConnect\n\nOur research question builds on this research to generalize the study to the U.S as a whole and determine risk of death rather than hospitalization alone. Our study will also seperate deaths by age categories to more accurately cater to patients."
  },
  {
    "objectID": "proposal.html#glimpse-of-data-2",
    "href": "proposal.html#glimpse-of-data-2",
    "title": "Tean AAAW Proposal",
    "section": "Glimpse of data",
    "text": "Glimpse of data\n\nAAAW_covid_cause <- read_csv('data/Conditions_Contributing_to_COVID-19_Deaths__by_State_and_Age__Provisional_2020-2023.csv')\n\nRows: 534060 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): Data As Of, Start Date, End Date, Group, State, Condition Group, C...\ndbl  (4): Year, Month, COVID-19 Deaths, Number of Mentions\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(AAAW_covid_cause)\n\nRows: 534,060\nColumns: 14\n$ `Data As Of`         <chr> \"02/26/2023\", \"02/26/2023\", \"02/26/2023\", \"02/26/…\n$ `Start Date`         <chr> \"01/01/2020\", \"01/01/2020\", \"01/01/2020\", \"01/01/…\n$ `End Date`           <chr> \"02/25/2023\", \"02/25/2023\", \"02/25/2023\", \"02/25/…\n$ Group                <chr> \"By Total\", \"By Total\", \"By Total\", \"By Total\", \"…\n$ Year                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Month                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ State                <chr> \"United States\", \"United States\", \"United States\"…\n$ `Condition Group`    <chr> \"Respiratory diseases\", \"Respiratory diseases\", \"…\n$ Condition            <chr> \"Influenza and pneumonia\", \"Influenza and pneumon…\n$ ICD10_codes          <chr> \"J09-J18\", \"J09-J18\", \"J09-J18\", \"J09-J18\", \"J09-…\n$ `Age Group`          <chr> \"0-24\", \"25-34\", \"35-44\", \"45-54\", \"55-64\", \"65-7…\n$ `COVID-19 Deaths`    <dbl> 1506, 5725, 14922, 37088, 81742, 126967, 135129, …\n$ `Number of Mentions` <dbl> 1577, 5942, 15538, 38541, 84727, 130934, 138351, …\n$ Flag                 <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…"
  },
  {
    "objectID": "report.html",
    "href": "report.html",
    "title": "Project title",
    "section": "",
    "text": "Introduction\nThe National Basketball Association (NBA) is the most popular premiere basketball league in the world. It produces data from every one of the 400-500 players in the league every season, leading to several million datapoints across NBA history. Every year, many models are developed to evaluate team strength and predict the success of the team.\nTeam depth is the overall quality of players in a sports team, especially those besides the team's star players. A team with good depth has competent players on both their starting lineup and their bench, which allows the team to withstand injuries and to have more flexibility for strategic adjustments. However, despite the importance of team depth, there is no established metric to measure it in the NBA. Quantitative analysis of the connection between team depth and team success is still missing. It is an often overlooked variable that significantly contributes to the over-all strength of the team. Performing an analysis on this question allow us to develop a systematic method to better understand and assess this often-neglected factor.\nIn this study, we investigate if the championship winning teams have a greater depth than the rest of the field using NBA play-off data from 1990-2022 collected by Robert Sunderhaft, who webscraped it from Basketball Reference, a public database of NBA statistics collated by Sports Reference LLC. In the dataset, we use 3 different methods to quantify team depth: (1) Basic statistics approach, (2) Advanced statistics, (3) Combo of Basic and Advanced statistics. We hypothesize that with all 3 measurements, the depths of championship winning teams would be significantly higher than those of the other teams because team depth has a positive effect on the team's performance. If this hypothesis holds true and the team depth does have an effect on championship, we will identify the quantification of team depth that resulted in the smallest p value (greatest significant difference). We will then fit this team depth metric to a logistic model to predict whether the team wins the championship, showing the real-life application of our statistical finding. \nThis research aims to develop a systematic way to evaluate team depth using the NBA play-off data from 1950-2022. We will then investigate its effect on the final results of the championship. To evaluate team depth, we will look at variables such as the points by bench players, numbers of bench players, field goal percentage of role players. We will also ask questions such as are the player minutes evenly-spreaded, and are all the positions well-subsituted, to evaluate the depth of the team. Eventually, we will assign relative scores for team depth and evaluate its association with final championship results.\nData background\nThe only ethical issue that arises with this use of this data set is that \"basketball skill\" is not a guaranteed, easily predictable variable and can change with each game. For example, a player can have an off day and thus have a dip in stats. That is why there is a high probability that any quality of play predictions made based off of these statistics is largely speculatory and could vary for any given game. \nAnother issue that arises is that some data, like percentages, blocks, turnovers, and other advanced statistics, weren’t collected for players early in this dataset, so there are null values instead.\nIn regards to this specific data set and the questions that we are addressing, the null hypothesis states that the true squad depth for champions minus the true squad depth for non-champions will equal zero, meaning that there is no statistical difference between the two measured teams. Our alternate hypothesis is that the true squad depth for champions minus the true squad depth for non-champions will be greater than zero, signifying a significant statistical relationship. This means that we can reject the null hypothesis if we were to get a p-value less than .05 and we cannot fail to reject it if it is greater than .05. \nLiterature Review: \nPublished credible article:  “Predicting the Outcome of NBA Playoffs Based on the Maximum Entropy Principle” (Cheng et al.)\nhttps://www.mdpi.com/1099-4300/18/12/450\nSummary\nCheng et al. developed a machine learning model that utilized maximum entropy principle to predict the outcome of nba play-off results using 14 basic technical features, such as 3 points, field goals, and number of steals. Their model reached an accuracy of 74.4%, which outperformed the previous state-of-the-art with 70% accuracy. The author reasoned that this increase of performance came from the maximum entropy principle as well as their consideration of the team’s last 6 game’s data which gave a more detailed depiction of team’s strength at the specific time before the play-off game.\nAmong the 14 features that they considered, team depth was not included. We believe that the team depth is a great factor that impacts the result of the play-off games. And if our hypothesis is true, the team depth variable could be a valuable addition to Cheng et al.’s machine learning to improve their accuracy.\n\n\n\nAAAW_playoff_stats <- read_csv(\"data/AAAW_updatedplayoffStats.csv\")\n\nRows: 10648 Columns: 51\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): player, pos, team_id\ndbl (48): season, age, g, gs, mp_per_g, fg_per_g, fga_per_g, fg_pct, fg3_per...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(AAAW_playoff_stats)\n\nRows: 10,648\nColumns: 51\n$ season           <dbl> 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022,…\n$ player           <chr> \"Omer Yurtseven\", \"Kessler Edwards\", \"Draymond Green\"…\n$ pos              <chr> \"C\", \"SF\", \"PF\", \"SF\", \"PG\", \"PF\", \"C\", \"PF\", \"SG\", \"…\n$ age              <dbl> 23, 21, 31, 34, 26, 26, 29, 33, 28, 23, 24, 28, 27, 2…\n$ team_id          <chr> \"MIA\", \"BRK\", \"GSW\", \"PHI\", \"NOP\", \"DEN\", \"UTA\", \"ATL…\n$ g                <dbl> 9, 2, 22, 12, 6, 5, 6, 5, 5, 6, 9, 18, 10, 6, 13, 4, …\n$ gs               <dbl> 0, 0, 22, 12, 0, 5, 6, 3, 0, 0, 0, 18, 10, 6, 13, 4, …\n$ mp_per_g         <dbl> 4.2, 3.5, 32.0, 26.6, 10.0, 32.0, 32.8, 22.4, 15.2, 6…\n$ fg_per_g         <dbl> 1.3, 0.0, 3.1, 3.0, 1.0, 4.6, 3.5, 3.6, 1.2, 0.0, 0.3…\n$ fga_per_g        <dbl> 2.0, 0.0, 6.5, 7.4, 3.0, 10.8, 5.5, 9.0, 3.0, 1.2, 0.…\n$ fg_pct           <dbl> 0.667, NA, 0.479, 0.404, 0.333, 0.426, 0.636, 0.400, …\n$ fg3_per_g        <dbl> 0.0, 0.0, 0.4, 2.6, 0.8, 0.6, 0.0, 0.8, 0.8, 0.0, 0.2…\n$ fg3a_per_g       <dbl> 0.2, 0.0, 1.8, 6.3, 2.3, 3.0, 0.0, 3.0, 2.2, 0.5, 0.2…\n$ fg3_pct          <dbl> 0.000, NA, 0.205, 0.408, 0.357, 0.200, NA, 0.267, 0.3…\n$ fg2_per_g        <dbl> 1.3, 0.0, 2.8, 0.4, 0.2, 4.0, 3.5, 2.8, 0.4, 0.0, 0.1…\n$ fg2a_per_g       <dbl> 1.8, 0.0, 4.8, 1.1, 0.7, 7.8, 5.5, 6.0, 0.8, 0.7, 0.2…\n$ fg2_pct          <dbl> 0.750, NA, 0.581, 0.385, 0.250, 0.513, 0.636, 0.467, …\n$ efg_pct          <dbl> 0.667, NA, 0.507, 0.579, 0.472, 0.454, 0.636, 0.444, …\n$ ft_per_g         <dbl> 0.1, 0.0, 1.4, 0.0, 1.2, 4.0, 5.0, 2.2, 0.8, 0.0, 0.0…\n$ fta_per_g        <dbl> 0.3, 0.0, 2.1, 0.1, 1.3, 5.6, 7.3, 2.2, 1.0, 0.0, 0.0…\n$ ft_pct           <dbl> 0.333, NA, 0.638, 0.000, 0.875, 0.714, 0.682, 1.000, …\n$ orb_per_g        <dbl> 0.1, 0.0, 1.2, 0.8, 0.3, 3.6, 3.5, 0.4, 0.2, 0.3, 0.0…\n$ drb_per_g        <dbl> 0.7, 0.0, 6.0, 2.3, 1.2, 3.6, 9.7, 3.8, 0.4, 0.2, 0.2…\n$ trb_per_g        <dbl> 0.8, 0.0, 7.2, 3.1, 1.5, 7.2, 13.2, 4.2, 0.6, 0.5, 0.…\n$ ast_per_g        <dbl> 0.3, 0.5, 6.3, 0.8, 0.7, 2.6, 0.5, 0.8, 1.4, 0.5, 0.0…\n$ stl_per_g        <dbl> 0.0, 0.5, 1.1, 1.0, 0.2, 0.4, 0.2, 0.2, 0.2, 0.2, 0.0…\n$ blk_per_g        <dbl> 0.1, 0.0, 1.0, 0.3, 0.2, 1.2, 1.0, 0.0, 0.0, 0.0, 0.0…\n$ tov_per_g        <dbl> 0.0, 0.5, 2.7, 1.1, 0.7, 1.6, 1.3, 1.0, 0.4, 0.2, 0.0…\n$ pf_per_g         <dbl> 0.2, 1.5, 4.0, 1.9, 0.5, 2.8, 3.2, 1.2, 1.4, 1.0, 0.0…\n$ pts_per_g        <dbl> 2.8, 0.0, 8.0, 8.6, 4.0, 13.8, 12.0, 10.2, 4.0, 0.0, …\n$ ast_pct          <dbl> 16.6, 17.5, 25.7, 4.9, 9.4, 12.0, 2.4, 6.4, 12.4, 10.…\n$ blk_pct          <dbl> 2.9, 0.0, 3.2, 1.2, 1.4, 3.9, 3.8, 0.0, 0.0, 0.0, 0.0…\n$ bpm              <dbl> 8.0, -4.4, 0.9, 1.3, -1.1, -1.3, 2.3, -1.8, -2.8, -14…\n$ dbpm             <dbl> 2.8, 6.4, 2.9, 0.9, -1.0, -2.2, 0.1, -1.3, -1.9, -2.9…\n$ drb_pct          <dbl> 19.3, 0.0, 20.6, 10.3, 14.7, 14.4, 34.1, 20.9, 3.4, 3…\n$ dws              <dbl> 0.0, 0.0, 1.0, 0.3, 0.0, 0.0, 0.2, 0.0, -0.1, 0.0, 0.…\n$ fg3a_per_fga_pct <dbl> 0.111, NA, 0.271, 0.854, 0.778, 0.278, 0.000, 0.333, …\n$ fta_per_fga_pct  <dbl> 0.167, NA, 0.326, 0.011, 0.444, 0.519, 1.333, 0.244, …\n$ mp               <dbl> 38, 7, 703, 319, 60, 160, 197, 112, 76, 36, 16, 688, …\n$ obpm             <dbl> 5.1, -10.8, -2.0, 0.4, -0.2, 0.9, 2.2, -0.6, -0.8, -1…\n$ orb_pct          <dbl> 3.0, 0.0, 4.3, 3.6, 3.7, 13.5, 12.5, 2.1, 1.6, 6.2, 0…\n$ ows              <dbl> 0.1, 0.0, 0.4, 0.0, 0.1, 0.3, 0.6, 0.0, 0.1, -0.1, 0.…\n$ per              <dbl> 25.8, -2.2, 12.3, 9.9, 11.5, 15.7, 19.4, 10.8, 7.8, -…\n$ stl_pct          <dbl> 0.0, 7.3, 1.8, 2.0, 0.9, 0.6, 0.3, 0.5, 0.7, 1.5, 0.0…\n$ tov_pct          <dbl> 0.0, 100.0, 26.4, 12.7, 15.7, 10.8, 13.3, 9.1, 10.4, …\n$ trb_pct          <dbl> 10.8, 0.0, 12.7, 7.1, 8.9, 13.9, 23.3, 11.3, 2.4, 5.1…\n$ ts_pct           <dbl> 0.647, NA, 0.534, 0.576, 0.558, 0.520, 0.688, 0.512, …\n$ usg_pct          <dbl> 22.6, 6.6, 14.0, 15.1, 18.3, 20.2, 14.3, 22.5, 11.0, …\n$ vorp             <dbl> 0.1, 0.0, 0.5, 0.3, 0.0, 0.0, 0.2, 0.0, 0.0, -0.1, 0.…\n$ ws               <dbl> 0.2, 0.0, 1.4, 0.3, 0.1, 0.2, 0.7, 0.0, 0.0, -0.2, 0.…\n$ ws_per_48        <dbl> 0.228, -0.104, 0.094, 0.049, 0.049, 0.063, 0.182, 0.0…\n\n\n\nNBA_df <- AAAW_playoff_stats |>\n  filter(season >= 1990)\n\n\nNBA_df <- NBA_df |>\n  mutate(team_id = paste(season, team_id, sep = \"\"))\n\nhead(NBA_df)\n\n# A tibble: 6 × 51\n  season player   pos     age team_id     g    gs mp_pe…¹ fg_pe…² fga_p…³ fg_pct\n   <dbl> <chr>    <chr> <dbl> <chr>   <dbl> <dbl>   <dbl>   <dbl>   <dbl>  <dbl>\n1   2022 Omer Yu… C        23 2022MIA     9     0     4.2     1.3     2    0.667\n2   2022 Kessler… SF       21 2022BRK     2     0     3.5     0       0   NA    \n3   2022 Draymon… PF       31 2022GSW    22    22    32       3.1     6.5  0.479\n4   2022 Danny G… SF       34 2022PHI    12    12    26.6     3       7.4  0.404\n5   2022 Devonte… PG       26 2022NOP     6     0    10       1       3    0.333\n6   2022 Aaron G… PF       26 2022DEN     5     5    32       4.6    10.8  0.426\n# … with 40 more variables: fg3_per_g <dbl>, fg3a_per_g <dbl>, fg3_pct <dbl>,\n#   fg2_per_g <dbl>, fg2a_per_g <dbl>, fg2_pct <dbl>, efg_pct <dbl>,\n#   ft_per_g <dbl>, fta_per_g <dbl>, ft_pct <dbl>, orb_per_g <dbl>,\n#   drb_per_g <dbl>, trb_per_g <dbl>, ast_per_g <dbl>, stl_per_g <dbl>,\n#   blk_per_g <dbl>, tov_per_g <dbl>, pf_per_g <dbl>, pts_per_g <dbl>,\n#   ast_pct <dbl>, blk_pct <dbl>, bpm <dbl>, dbpm <dbl>, drb_pct <dbl>,\n#   dws <dbl>, fg3a_per_fga_pct <dbl>, fta_per_fga_pct <dbl>, mp <dbl>, …\n\n\n\nNBA_df <- NBA_df |>\n  filter(g >= 4,\n         mp_per_g >= 10)\n\nPart 1: Basic stats\n\nNBA_df_basic <- NBA_df |>\n  mutate(pp_36 = pts_per_g / mp_per_g * 36,\n         rp_36 = trb_per_g / mp_per_g * 36,\n         ap_36 = ast_per_g / mp_per_g * 36)\n\n\nNBA_df_basic |>\n  ggplot(aes(x = pp_36)) +\n  geom_histogram(binwidth = 1)\n\n\n\nNBA_df_basic |>\n  ggplot(aes(x = rp_36)) +\n  geom_histogram(binwidth = 0.5)\n\n\n\nNBA_df_basic |>\n  ggplot(aes(x = ap_36)) +\n  geom_histogram(binwidth = 0.2)\n\n\n\n\n\nQual_player_df_basic <- NBA_df_basic |>\n  filter(pp_36 > 16 | rp_36 > 6.5 | ap_36 > 3.5) |>\n  group_by(team_id) |>\n  summarise(count_qual = n())\n\n\nAll_rot_players_df <- NBA_df |>\n  group_by(team_id) |>\n  summarise(count_rot = n())\n\nBasic_df <- Qual_player_df_basic |>\n  left_join(All_rot_players_df, by = \"team_id\")\n\n\nBasic_df <- Basic_df |>\n  mutate(Basic_depth = count_qual / count_rot,\n         Championship = if_else(team_id %in% c(\"1990DET\", \"1991CHI\", \"1992CHI\", \"1993CHI\", \"1994HOU\", \"1995HOU\", \"1996CHI\", \"1997CHI\", \"1998CHI\", \"1999SAS\", \"2000LAL\", \"2001LAL\", \"2002LAL\", \"2003SAS\", \"2004DET\", \"2005SAS\", \"2006MIA\", \"2007SAS\", \"2008BOS\", \"2009LAL\", \"2010LAL\", \"2011DAL\", \"2012MIA\", \"2013MIA\", \"2014SAS\", \"2015GSW\", \"2016CLE\", \"2017GSW\", \"2018GSW\", \"2019TOR\", \"2020LAL\", \"2021MIL\", \"2022GSW\"), \"Champ\", \"No Champ\" ))\n\n\nBasic_df |>\n  group_by(Championship) |>\n  summarize(mean(Basic_depth))\n\n# A tibble: 2 × 2\n  Championship `mean(Basic_depth)`\n  <chr>                      <dbl>\n1 Champ                      0.755\n2 No Champ                   0.720\n\n\n\nBasic_df |>\n  ggplot(\n    aes(x = Basic_depth, y = Championship, fill = Championship)\n  ) +\n  geom_boxplot() +\n  theme(legend.position = \"none\")\n\n\n\n\nPart 2: Advanced Stats\n\nQual_player_df_adv <- NBA_df |>\n  filter(per >= 15 | ws_per_48 >= 0.1) |>\n  group_by(team_id) |>\n  summarise(count_qual = n())\n\nAdv_df <- Qual_player_df_adv |>\n  left_join(All_rot_players_df, by = \"team_id\")\n\n\nAdv_df <- Adv_df |>\n  mutate(Adv_depth = count_qual / count_rot,\n         Championship = if_else(team_id %in% c(\"1990DET\", \"1991CHI\", \"1992CHI\", \"1993CHI\", \"1994HOU\", \"1995HOU\", \"1996CHI\", \"1997CHI\", \"1998CHI\", \"1999SAS\", \"2000LAL\", \"2001LAL\", \"2002LAL\", \"2003SAS\", \"2004DET\", \"2005SAS\", \"2006MIA\", \"2007SAS\", \"2008BOS\", \"2009LAL\", \"2010LAL\", \"2011DAL\", \"2012MIA\", \"2013MIA\", \"2014SAS\", \"2015GSW\", \"2016CLE\", \"2017GSW\", \"2018GSW\", \"2019TOR\", \"2020LAL\", \"2021MIL\", \"2022GSW\"), \"Champ\", \"No Champ\" ))\n\n\nAdv_df |>\n  group_by(Championship) |>\n  summarize(mean(Adv_depth))\n\n# A tibble: 2 × 2\n  Championship `mean(Adv_depth)`\n  <chr>                    <dbl>\n1 Champ                    0.751\n2 No Champ                 0.506\n\n\n\nAdv_df |>\n  ggplot(\n    aes(x = Adv_depth, y = Championship, fill = Championship)\n  ) +\n  geom_boxplot() +\n  theme(legend.position = \"none\")\n\n\n\n\nPart 3: Combo\n\nNBA_df_combo <- NBA_df |>\n  mutate(pp_36 = pts_per_g / mp_per_g * 36,\n         rp_36 = trb_per_g / mp_per_g * 36,\n         ap_36 = ast_per_g / mp_per_g * 36)\n\nQual_player_df_combo <- NBA_df_combo |>\n  filter(pp_36 > 16 | rp_36 > 6.5 | ap_36 > 3.5, \n         per >= 15 | ws_per_48 >= 0.1) |>\n  group_by(team_id) |>\n  summarise(count_qual = n())\n\nCombo_df <- Qual_player_df_combo |>\n  left_join(All_rot_players_df, by = \"team_id\")\n\n\nCombo_df <- Combo_df |>\n  mutate(Combo_depth = count_qual / count_rot,\n         Championship = if_else(team_id %in% c(\"1990DET\", \"1991CHI\", \"1992CHI\", \"1993CHI\", \"1994HOU\", \"1995HOU\", \"1996CHI\", \"1997CHI\", \"1998CHI\", \"1999SAS\", \"2000LAL\", \"2001LAL\", \"2002LAL\", \"2003SAS\", \"2004DET\", \"2005SAS\", \"2006MIA\", \"2007SAS\", \"2008BOS\", \"2009LAL\", \"2010LAL\", \"2011DAL\", \"2012MIA\", \"2013MIA\", \"2014SAS\", \"2015GSW\", \"2016CLE\", \"2017GSW\", \"2018GSW\", \"2019TOR\", \"2020LAL\", \"2021MIL\", \"2022GSW\"), \"Champ\", \"No Champ\" ))\n\n\nCombo_df |>\n  group_by(Championship) |>\n  summarize(mean(Combo_depth))\n\n# A tibble: 2 × 2\n  Championship `mean(Combo_depth)`\n  <chr>                      <dbl>\n1 Champ                      0.627\n2 No Champ                   0.439\n\n\n\nCombo_df |>\n  ggplot(\n    aes(x = Combo_depth, y = Championship, fill = Championship)\n  ) +\n  geom_boxplot() +\n  theme(legend.position = \"none\")"
=======
    "text": "AAAW_playoff_stats <- read_csv(\"data/AAAW_updatedplayoffStats.csv\")\n\nRows: 10648 Columns: 51\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): player, pos, team_id\ndbl (48): season, age, g, gs, mp_per_g, fg_per_g, fga_per_g, fg_pct, fg3_per...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(AAAW_playoff_stats)\n\nRows: 10,648\nColumns: 51\n$ season           <dbl> 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022,…\n$ player           <chr> \"Omer Yurtseven\", \"Kessler Edwards\", \"Draymond Green\"…\n$ pos              <chr> \"C\", \"SF\", \"PF\", \"SF\", \"PG\", \"PF\", \"C\", \"PF\", \"SG\", \"…\n$ age              <dbl> 23, 21, 31, 34, 26, 26, 29, 33, 28, 23, 24, 28, 27, 2…\n$ team_id          <chr> \"MIA\", \"BRK\", \"GSW\", \"PHI\", \"NOP\", \"DEN\", \"UTA\", \"ATL…\n$ g                <dbl> 9, 2, 22, 12, 6, 5, 6, 5, 5, 6, 9, 18, 10, 6, 13, 4, …\n$ gs               <dbl> 0, 0, 22, 12, 0, 5, 6, 3, 0, 0, 0, 18, 10, 6, 13, 4, …\n$ mp_per_g         <dbl> 4.2, 3.5, 32.0, 26.6, 10.0, 32.0, 32.8, 22.4, 15.2, 6…\n$ fg_per_g         <dbl> 1.3, 0.0, 3.1, 3.0, 1.0, 4.6, 3.5, 3.6, 1.2, 0.0, 0.3…\n$ fga_per_g        <dbl> 2.0, 0.0, 6.5, 7.4, 3.0, 10.8, 5.5, 9.0, 3.0, 1.2, 0.…\n$ fg_pct           <dbl> 0.667, NA, 0.479, 0.404, 0.333, 0.426, 0.636, 0.400, …\n$ fg3_per_g        <dbl> 0.0, 0.0, 0.4, 2.6, 0.8, 0.6, 0.0, 0.8, 0.8, 0.0, 0.2…\n$ fg3a_per_g       <dbl> 0.2, 0.0, 1.8, 6.3, 2.3, 3.0, 0.0, 3.0, 2.2, 0.5, 0.2…\n$ fg3_pct          <dbl> 0.000, NA, 0.205, 0.408, 0.357, 0.200, NA, 0.267, 0.3…\n$ fg2_per_g        <dbl> 1.3, 0.0, 2.8, 0.4, 0.2, 4.0, 3.5, 2.8, 0.4, 0.0, 0.1…\n$ fg2a_per_g       <dbl> 1.8, 0.0, 4.8, 1.1, 0.7, 7.8, 5.5, 6.0, 0.8, 0.7, 0.2…\n$ fg2_pct          <dbl> 0.750, NA, 0.581, 0.385, 0.250, 0.513, 0.636, 0.467, …\n$ efg_pct          <dbl> 0.667, NA, 0.507, 0.579, 0.472, 0.454, 0.636, 0.444, …\n$ ft_per_g         <dbl> 0.1, 0.0, 1.4, 0.0, 1.2, 4.0, 5.0, 2.2, 0.8, 0.0, 0.0…\n$ fta_per_g        <dbl> 0.3, 0.0, 2.1, 0.1, 1.3, 5.6, 7.3, 2.2, 1.0, 0.0, 0.0…\n$ ft_pct           <dbl> 0.333, NA, 0.638, 0.000, 0.875, 0.714, 0.682, 1.000, …\n$ orb_per_g        <dbl> 0.1, 0.0, 1.2, 0.8, 0.3, 3.6, 3.5, 0.4, 0.2, 0.3, 0.0…\n$ drb_per_g        <dbl> 0.7, 0.0, 6.0, 2.3, 1.2, 3.6, 9.7, 3.8, 0.4, 0.2, 0.2…\n$ trb_per_g        <dbl> 0.8, 0.0, 7.2, 3.1, 1.5, 7.2, 13.2, 4.2, 0.6, 0.5, 0.…\n$ ast_per_g        <dbl> 0.3, 0.5, 6.3, 0.8, 0.7, 2.6, 0.5, 0.8, 1.4, 0.5, 0.0…\n$ stl_per_g        <dbl> 0.0, 0.5, 1.1, 1.0, 0.2, 0.4, 0.2, 0.2, 0.2, 0.2, 0.0…\n$ blk_per_g        <dbl> 0.1, 0.0, 1.0, 0.3, 0.2, 1.2, 1.0, 0.0, 0.0, 0.0, 0.0…\n$ tov_per_g        <dbl> 0.0, 0.5, 2.7, 1.1, 0.7, 1.6, 1.3, 1.0, 0.4, 0.2, 0.0…\n$ pf_per_g         <dbl> 0.2, 1.5, 4.0, 1.9, 0.5, 2.8, 3.2, 1.2, 1.4, 1.0, 0.0…\n$ pts_per_g        <dbl> 2.8, 0.0, 8.0, 8.6, 4.0, 13.8, 12.0, 10.2, 4.0, 0.0, …\n$ ast_pct          <dbl> 16.6, 17.5, 25.7, 4.9, 9.4, 12.0, 2.4, 6.4, 12.4, 10.…\n$ blk_pct          <dbl> 2.9, 0.0, 3.2, 1.2, 1.4, 3.9, 3.8, 0.0, 0.0, 0.0, 0.0…\n$ bpm              <dbl> 8.0, -4.4, 0.9, 1.3, -1.1, -1.3, 2.3, -1.8, -2.8, -14…\n$ dbpm             <dbl> 2.8, 6.4, 2.9, 0.9, -1.0, -2.2, 0.1, -1.3, -1.9, -2.9…\n$ drb_pct          <dbl> 19.3, 0.0, 20.6, 10.3, 14.7, 14.4, 34.1, 20.9, 3.4, 3…\n$ dws              <dbl> 0.0, 0.0, 1.0, 0.3, 0.0, 0.0, 0.2, 0.0, -0.1, 0.0, 0.…\n$ fg3a_per_fga_pct <dbl> 0.111, NA, 0.271, 0.854, 0.778, 0.278, 0.000, 0.333, …\n$ fta_per_fga_pct  <dbl> 0.167, NA, 0.326, 0.011, 0.444, 0.519, 1.333, 0.244, …\n$ mp               <dbl> 38, 7, 703, 319, 60, 160, 197, 112, 76, 36, 16, 688, …\n$ obpm             <dbl> 5.1, -10.8, -2.0, 0.4, -0.2, 0.9, 2.2, -0.6, -0.8, -1…\n$ orb_pct          <dbl> 3.0, 0.0, 4.3, 3.6, 3.7, 13.5, 12.5, 2.1, 1.6, 6.2, 0…\n$ ows              <dbl> 0.1, 0.0, 0.4, 0.0, 0.1, 0.3, 0.6, 0.0, 0.1, -0.1, 0.…\n$ per              <dbl> 25.8, -2.2, 12.3, 9.9, 11.5, 15.7, 19.4, 10.8, 7.8, -…\n$ stl_pct          <dbl> 0.0, 7.3, 1.8, 2.0, 0.9, 0.6, 0.3, 0.5, 0.7, 1.5, 0.0…\n$ tov_pct          <dbl> 0.0, 100.0, 26.4, 12.7, 15.7, 10.8, 13.3, 9.1, 10.4, …\n$ trb_pct          <dbl> 10.8, 0.0, 12.7, 7.1, 8.9, 13.9, 23.3, 11.3, 2.4, 5.1…\n$ ts_pct           <dbl> 0.647, NA, 0.534, 0.576, 0.558, 0.520, 0.688, 0.512, …\n$ usg_pct          <dbl> 22.6, 6.6, 14.0, 15.1, 18.3, 20.2, 14.3, 22.5, 11.0, …\n$ vorp             <dbl> 0.1, 0.0, 0.5, 0.3, 0.0, 0.0, 0.2, 0.0, 0.0, -0.1, 0.…\n$ ws               <dbl> 0.2, 0.0, 1.4, 0.3, 0.1, 0.2, 0.7, 0.0, 0.0, -0.2, 0.…\n$ ws_per_48        <dbl> 0.228, -0.104, 0.094, 0.049, 0.049, 0.063, 0.182, 0.0…\n\n\n\nNBA_df <- AAAW_playoff_stats |>\n  filter(season >= 1990)\n\n\nNBA_df <- NBA_df |>\n  mutate(team_id = paste(season, team_id, sep = \"\"))\n\nhead(NBA_df)\n\n# A tibble: 6 × 51\n  season player   pos     age team_id     g    gs mp_pe…¹ fg_pe…² fga_p…³ fg_pct\n   <dbl> <chr>    <chr> <dbl> <chr>   <dbl> <dbl>   <dbl>   <dbl>   <dbl>  <dbl>\n1   2022 Omer Yu… C        23 2022MIA     9     0     4.2     1.3     2    0.667\n2   2022 Kessler… SF       21 2022BRK     2     0     3.5     0       0   NA    \n3   2022 Draymon… PF       31 2022GSW    22    22    32       3.1     6.5  0.479\n4   2022 Danny G… SF       34 2022PHI    12    12    26.6     3       7.4  0.404\n5   2022 Devonte… PG       26 2022NOP     6     0    10       1       3    0.333\n6   2022 Aaron G… PF       26 2022DEN     5     5    32       4.6    10.8  0.426\n# … with 40 more variables: fg3_per_g <dbl>, fg3a_per_g <dbl>, fg3_pct <dbl>,\n#   fg2_per_g <dbl>, fg2a_per_g <dbl>, fg2_pct <dbl>, efg_pct <dbl>,\n#   ft_per_g <dbl>, fta_per_g <dbl>, ft_pct <dbl>, orb_per_g <dbl>,\n#   drb_per_g <dbl>, trb_per_g <dbl>, ast_per_g <dbl>, stl_per_g <dbl>,\n#   blk_per_g <dbl>, tov_per_g <dbl>, pf_per_g <dbl>, pts_per_g <dbl>,\n#   ast_pct <dbl>, blk_pct <dbl>, bpm <dbl>, dbpm <dbl>, drb_pct <dbl>,\n#   dws <dbl>, fg3a_per_fga_pct <dbl>, fta_per_fga_pct <dbl>, mp <dbl>, …\n\n\n\nNBA_df <- NBA_df |>\n  filter(g >= 4,\n         mp_per_g >= 10)\n\nPart 1: Basic stats\n\nNBA_df_basic <- NBA_df |>\n  mutate(pp_36 = pts_per_g / mp_per_g * 36,\n         rp_36 = trb_per_g / mp_per_g * 36,\n         ap_36 = ast_per_g / mp_per_g * 36)\n\n\nNBA_df_basic |>\n  ggplot(aes(x = pp_36)) +\n  geom_histogram(binwidth = 1)\n\n\n\nNBA_df_basic |>\n  ggplot(aes(x = rp_36)) +\n  geom_histogram(binwidth = 0.5)\n\n\n\nNBA_df_basic |>\n  ggplot(aes(x = ap_36)) +\n  geom_histogram(binwidth = 0.2)\n\n\n\n\n\nQual_player_df_basic <- NBA_df_basic |>\n  filter(pp_36 > 16 | rp_36 > 6.5 | ap_36 > 3.5) |>\n  group_by(team_id) |>\n  summarise(count_qual = n())\n\n\nAll_rot_players_df <- NBA_df |>\n  group_by(team_id) |>\n  summarise(count_rot = n())\n\nBasic_df <- Qual_player_df_basic |>\n  left_join(All_rot_players_df, by = \"team_id\")\n\nBasic_df\n\n# A tibble: 496 × 3\n   team_id count_qual count_rot\n   <chr>        <int>     <int>\n 1 1990BOS          7         8\n 2 1990CHI          7         9\n 3 1990CLE          5         7\n 4 1990DET          8         8\n 5 1990HOU          5         7\n 6 1990LAL          6         8\n 7 1990MIL          6         7\n 8 1990NYK          6         8\n 9 1990PHI          5         7\n10 1990PHO          6         7\n# … with 486 more rows\n\n\n\nBasic_df <- Basic_df |>\n  mutate(Basic_depth = count_qual / count_rot,\n         Championship = if_else(team_id %in% c(\"1990DET\", \"1991CHI\", \"1992CHI\", \"1993CHI\", \"1994HOU\", \"1995HOU\", \"1996CHI\", \"1997CHI\", \"1998CHI\", \"1999SAS\", \"2000LAL\", \"2001LAL\", \"2002LAL\", \"2003SAS\", \"2004DET\", \"2005SAS\", \"2006MIA\", \"2007SAS\", \"2008BOS\", \"2009LAL\", \"2010LAL\", \"2011DAL\", \"2012MIA\", \"2013MIA\", \"2014SAS\", \"2015GSW\", \"2016CLE\", \"2017GSW\", \"2018GSW\", \"2019TOR\", \"2020LAL\", \"2021MIL\", \"2022GSW\"), \"Champ\", \"Not Champ\" ))\n\n\nBasic_df |>\n  ggplot(\n    aes(x = Championship, y = Basic_depth,  fill = Championship)\n  ) +\n  geom_boxplot() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Basic Depth of Champions vs Non-Champions\",\n       x = \"Team\",\n       y = \"Team Depth (Defined by Basic Stats)\")\n\n\n\n\n\nBasic_df |>\n  ggplot(\n    aes(x = Championship, y = Basic_depth,  fill = Championship)\n  ) +\n  geom_violin() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Basic Depth of Champions vs Non-Champions\",\n       x = \"Team\",\n       y = \"Team Depth (Defined by Basic Stats)\")\n\n\n\n\nStatistical Test\n\nBasic_df |>\n  group_by(Championship) |>\n  summarize(mean(Basic_depth))\n\n# A tibble: 2 × 2\n  Championship `mean(Basic_depth)`\n  <chr>                      <dbl>\n1 Champ                      0.755\n2 Not Champ                  0.720\n\n\n\\(\\bar{x_c} - \\bar{x_n}\\) = 0.04\n\nnull_dist_basic <- Basic_df |>\n  specify(response = Basic_depth, explanatory = Championship) |>\n  hypothesize(null = \"independence\") |>\n  generate(reps = 1000, type = \"permute\") |>\n  calculate(stat = \"diff in means\", order = c(\"Champ\", \"Not Champ\"))\n\n\nvisualize(null_dist_basic) +\n shade_p_value(obs_stat = 0.04, direction = \"greater\") +\n  labs(title = \"Null Distribution of (Champion Depth - Non-Champion Depth) \nusing Basic Statistics\",\n      x = \"Difference (Champion Depth - Non-Champion Depth)\",\n      y = \"Frequency\")\n\n\n\n\n\nnull_dist_basic |>\n  get_p_value(obs_stat = 0.04, direction = \"right\")\n\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1   0.059\n\n\nPart 2: Advanced Stats\n\nQual_player_df_adv <- NBA_df |>\n  filter(per >= 15 | ws_per_48 >= 0.1) |>\n  group_by(team_id) |>\n  summarise(count_qual = n())\n\nAdv_df <- Qual_player_df_adv |>\n  left_join(All_rot_players_df, by = \"team_id\")\n\n\nAdv_df <- Adv_df |>\n  mutate(Adv_depth = count_qual / count_rot,\n         Championship = if_else(team_id %in% c(\"1990DET\", \"1991CHI\", \"1992CHI\", \"1993CHI\", \"1994HOU\", \"1995HOU\", \"1996CHI\", \"1997CHI\", \"1998CHI\", \"1999SAS\", \"2000LAL\", \"2001LAL\", \"2002LAL\", \"2003SAS\", \"2004DET\", \"2005SAS\", \"2006MIA\", \"2007SAS\", \"2008BOS\", \"2009LAL\", \"2010LAL\", \"2011DAL\", \"2012MIA\", \"2013MIA\", \"2014SAS\", \"2015GSW\", \"2016CLE\", \"2017GSW\", \"2018GSW\", \"2019TOR\", \"2020LAL\", \"2021MIL\", \"2022GSW\"), \"Champ\", \"Not Champ\" ))\n\n\nAdv_df |>\n  ggplot(\n    aes(x = Championship, y = Adv_depth,  fill = Championship)\n  ) +\n  geom_boxplot() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Advanved Depth of Champions vs Non-Champions\",\n       x = \"Team\",\n       y = \"Team Depth (Defined by Adv Stats)\")\n\n\n\n\n\nAdv_df |>\n  ggplot(\n    aes(x = Championship, y = Adv_depth,  fill = Championship)\n  ) +\n  geom_violin() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Advanved Depth of Champions vs Non-Champions\",\n       x = \"Team\",\n       y = \"Team Depth (Defined by Adv Stats)\")\n\n\n\n\n\nAdv_df |>\n  group_by(Championship) |>\n  summarize(mean(Adv_depth))\n\n# A tibble: 2 × 2\n  Championship `mean(Adv_depth)`\n  <chr>                    <dbl>\n1 Champ                    0.751\n2 Not Champ                0.506\n\n\n\\(\\bar{x_c} - \\bar{x_n}\\) = 0.25\n\nnull_dist_adv <- Adv_df |>\n  specify(response = Adv_depth, explanatory = Championship) |>\n  hypothesize(null = \"independence\") |>\n  generate(reps = 1000, type = \"permute\") |>\n  calculate(stat = \"diff in means\", order = c(\"Champ\", \"Not Champ\"))\n\n\nvisualize(null_dist_adv) +\n shade_p_value(obs_stat = 0.25, direction = \"greater\") +\n  labs(title = \"Advanced Distribution of (Champion Depth - Non-Champion Depth) \nusing Basic Statistics\",\n      x = \"Difference (Champion Depth - Non-Champion Depth)\",\n      y = \"Frequency\")\n\nWarning in min(diff(unique_loc)): no non-missing arguments to min; returning Inf\n\n\n\n\n\n\nnull_dist_adv |>\n  get_p_value(obs_stat = 0.25, direction = \"right\")\n\nWarning: Please be cautious in reporting a p-value of 0. This result is an\napproximation based on the number of `reps` chosen in the `generate()` step. See\n`?get_p_value()` for more information.\n\n\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1       0\n\n\nPart 3: Combo\n\nNBA_df_combo <- NBA_df |>\n  mutate(pp_36 = pts_per_g / mp_per_g * 36,\n         rp_36 = trb_per_g / mp_per_g * 36,\n         ap_36 = ast_per_g / mp_per_g * 36)\n\nQual_player_df_combo <- NBA_df_combo |>\n  filter(pp_36 > 16 | rp_36 > 6.5 | ap_36 > 3.5, \n         per >= 15 | ws_per_48 >= 0.1) |>\n  group_by(team_id) |>\n  summarise(count_qual = n())\n\nCombo_df <- Qual_player_df_combo |>\n  left_join(All_rot_players_df, by = \"team_id\")\n\n\nCombo_df <- Combo_df |>\n  mutate(Combo_depth = count_qual / count_rot,\n         Championship = if_else(team_id %in% c(\"1990DET\", \"1991CHI\", \"1992CHI\", \"1993CHI\", \"1994HOU\", \"1995HOU\", \"1996CHI\", \"1997CHI\", \"1998CHI\", \"1999SAS\", \"2000LAL\", \"2001LAL\", \"2002LAL\", \"2003SAS\", \"2004DET\", \"2005SAS\", \"2006MIA\", \"2007SAS\", \"2008BOS\", \"2009LAL\", \"2010LAL\", \"2011DAL\", \"2012MIA\", \"2013MIA\", \"2014SAS\", \"2015GSW\", \"2016CLE\", \"2017GSW\", \"2018GSW\", \"2019TOR\", \"2020LAL\", \"2021MIL\", \"2022GSW\"), \"Champ\", \"Not Champ\" ))\n\n\nCombo_df |>\n  ggplot(\n    aes(x = Championship, y = Combo_depth,  fill = Championship)\n  ) +\n  geom_boxplot() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Combo Depth of Champions vs Non-Champions\",\n       x = \"Team\",\n       y = \"Team Depth (Defined by Combo Stats)\")\n\n\n\n\n\nCombo_df |>\n  ggplot(\n    aes(x = Championship, y = Combo_depth,  fill = Championship)\n  ) +\n  geom_violin() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Combo Depth of Champions vs Non-Champions\",\n       x = \"Team\",\n       y = \"Team Depth (Defined by Combo Stats)\")\n\n\n\n\n\nCombo_df |>\n  group_by(Championship) |>\n  summarize(mean(Combo_depth))\n\n# A tibble: 2 × 2\n  Championship `mean(Combo_depth)`\n  <chr>                      <dbl>\n1 Champ                      0.627\n2 Not Champ                  0.439\n\n\n\\(\\bar{x_c} - \\bar{x_n}\\) = 0.19\n\nnull_dist_combo <- Combo_df |>\n  specify(response = Combo_depth, explanatory = Championship) |>\n  hypothesize(null = \"independence\") |>\n  generate(reps = 1000, type = \"permute\") |>\n  calculate(stat = \"diff in means\", order = c(\"Champ\", \"Not Champ\"))\n\n\nvisualize(null_dist_combo) +\n shade_p_value(obs_stat = 0.19, direction = \"greater\") +\n  labs(title = \"Combo Distribution of (Champion Depth - Non-Champion Depth) \nusing Basic Statistics\",\n      x = \"Difference (Champion Depth - Non-Champion Depth)\",\n      y = \"Frequency\")\n\nWarning in min(diff(unique_loc)): no non-missing arguments to min; returning Inf\n\n\n\n\n\n\nnull_dist_combo |>\n  get_p_value(obs_stat = 0.19, direction = \"right\")\n\nWarning: Please be cautious in reporting a p-value of 0. This result is an\napproximation based on the number of `reps` chosen in the `generate()` step. See\n`?get_p_value()` for more information.\n\n\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1       0\n\n\nModel Fitting\n\nAdv_df <- Adv_df |>\n  mutate (Championship = as.factor(Championship))\n\nChamp_model <- logistic_reg() |>\n  set_engine('glm') |>\n  fit(Championship ~ Adv_depth, data = Adv_df, family = 'binomial') \n\n\nChamp_model |>\n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)     8.69      1.06      8.21 2.25e-16\n2 Adv_depth      -9.55      1.45     -6.58 4.77e-11\n\n\n\nAdv_df <- Adv_df |>\n  mutate(pred_prob = predict(Champ_model$fit, type = \"response\"))\n\nggplot(data = Adv_df) + \n  geom_point(aes(x = Adv_depth, y = as.numeric(Championship) -1, \n                        color = Championship)) + \n  geom_line(aes(x = Adv_depth, y = pred_prob)) + \n  labs(title = \"Logistic Model of Adv_depth on Championship\",\n       x = \"Team Depth from Advanced Statistics\", \n       y = \"Probability of Team Winning Championship\", \n       color = \"Champion or Not\"\n       )"
  }
]